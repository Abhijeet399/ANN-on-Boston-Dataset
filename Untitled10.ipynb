{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "df = pd.read_csv('C:/Users/bhatt/Desktop/ML_DATASET/data1.csv')\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/bhatt/Desktop/ML_DATASET/data1.csv', header=None)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df\n",
    "y = pd.read_csv('C:/Users/bhatt/Desktop/ML_DATASET/label1.csv', header=None)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.44689111 -0.01339702  0.23264493 ... -0.94587809  1.84092189\n",
      "  -0.27902404]\n",
      " [ 0.24374694 -1.14417545 -0.62221358 ...  0.90293107 -1.41872787\n",
      "  -0.29013935]\n",
      " [-1.41779879 -0.0888327  -0.647181   ...  1.20946469 -1.52638614\n",
      "   0.33777109]\n",
      " ...\n",
      " [-0.80040466  1.47799654  0.0900574  ...  0.37866958  0.11096771\n",
      "  -0.17636265]\n",
      " [-1.21734449 -0.78540102 -0.41378073 ...  0.24927734 -0.94149613\n",
      "   1.66166061]\n",
      " [ 1.60100517 -0.89039824 -1.42504908 ... -0.02377265  1.2208092\n",
      "   1.48933029]]\n"
     ]
    }
   ],
   "source": [
    "x_array=np.array(x)\n",
    "print(x_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 20)\n"
     ]
    }
   ],
   "source": [
    "print(x_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(x_array.size)\n",
    "print(x_array.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]\n",
      " [2]\n",
      " [2]\n",
      " ...\n",
      " [2]\n",
      " [2]\n",
      " [0]]\n",
      "(20000, 1)\n",
      "20000\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "y_array=np.array(y)\n",
    "print(y_array)\n",
    "print(y_array.shape)\n",
    "print(y_array.size)\n",
    "print(y_array.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4\n",
      "dtype: int64\n",
      "0    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y.max())\n",
    "print(y.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(np.eye((np.max(y_array)+1))[y_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "y_train=np.eye((np.max(y_array)+1))[y_array]\n",
    "print(y_train.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1, 5)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "y_train=np.reshape(y_train,(20000,5))\n",
    "print(y_train.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 20)\n",
      "(2000, 20)\n",
      "(2000, 20)\n",
      "(16000, 5)\n",
      "(2000, 5)\n",
      "(2000, 5)\n"
     ]
    }
   ],
   "source": [
    "xtrain=x_array[0:16000]\n",
    "xtest=x_array[16000:18000]\n",
    "xvalidate=x_array[18000:20000]\n",
    "ytrain=y_train[0:16000]\n",
    "ytest=y_train[16000:18000]\n",
    "yvalidate=y_train[18000:20000]\n",
    "print(xtrain.shape)\n",
    "print(xtest.shape)\n",
    "print(xvalidate.shape)\n",
    "print(ytrain.shape)\n",
    "print(ytest.shape)\n",
    "print(yvalidate.shape)\n",
    "finalcost=0\n",
    "cost=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10)\n",
      "(10, 5)\n",
      "(10,)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "w1 = np.random.normal(0, 0.2236, (20, 10))\n",
    "#print(w1)\n",
    "w2= np.random.normal(0, 0.2236, (10 , 5))\n",
    "#print(w2)\n",
    "b1=np.zeros(10)\n",
    "b2=np.zeros(5)\n",
    "print(w1.shape)\n",
    "print(w2.shape)\n",
    "print(b1.shape)\n",
    "print(b2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actFunc(haha,choose):\n",
    "    if (choose==1):#Sigmoid\n",
    "        return (1/(1+np.exp(haha)))\n",
    "    elif (choose==2):#ReLU\n",
    "        return np.max(0,haha)\n",
    "    elif (choose==3):#tanh\n",
    "        return np.tanh(haha)\n",
    "    elif (choose==4):#softmax\n",
    "        temp=np.exp(haha)\n",
    "        return temp/np.sum(temp,axis=1,keepdims=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate0.001\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "alpha=float(input(\"learning rate\"))\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size1000\n"
     ]
    }
   ],
   "source": [
    "batch=int(input(\"Batch size\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(y,t):\n",
    "    return -(t*(np.log(y))+(1-t)*np.log(1-y))\n",
    "    #return -(t*(math.log10(y))+(1-t)*(math.log10(1-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output,labelData,n):\n",
    "    maxInRow=np.argmax(output, axis=1)\n",
    "    maxInLabels=np.argmax(labelData, axis=1)\n",
    "    huehue=np.equal(maxInRow,maxInLabels)\n",
    "    count=np.sum(huehue)\n",
    "    #count=0\n",
    "    #for l in range(16000):\n",
    "    #    if maxInRow[l]==maxInLabels[l]:\n",
    "    #        count=count+1\n",
    "    return (count/n)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardpass(mattrix):\n",
    "    a0=mattrix\n",
    "    z1=np.dot(a0,w1)+b1\n",
    "    a1=actFunc(z1,3)\n",
    "    z2=np.dot(a1,w2)+b2\n",
    "    a2=actFunc(z2,3)\n",
    "    return a0,z1,a1,z2,a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 20)\n",
      "(16000, 5)\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# print(xtrain[0])\n",
    "# print(ytrain)\n",
    "print(xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "print(xtrain.ndim)\n",
    "print(ytrain.ndim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhatt\\Anaconda3\\envs\\virenv\\lib\\site-packages\\ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in log\n",
      "C:\\Users\\bhatt\\Anaconda3\\envs\\virenv\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in log\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost\n",
      "nan\n",
      "[[-0.35832876 -0.63134161         nan  1.14620415 -0.08327269]\n",
      " [        nan -0.27419292  0.65072981 -0.06635093 -0.47542017]\n",
      " [        nan -0.09781889  0.29202235 -0.02723376 -0.21085793]\n",
      " ...\n",
      " [-0.55992941 -0.28092009         nan  1.02866702 -0.09164791]\n",
      " [-0.04297551 -0.12615847 -0.36924553 -0.01167341  0.69205753]\n",
      " [        nan -0.19488939 -0.19137062 -0.03239158  0.49322551]]\n",
      "training accuracy\n",
      "73.16875\n",
      "cost\n",
      "nan\n",
      "[[-0.40122118 -0.55948254         nan  1.08583394 -0.03616743]\n",
      " [        nan -0.14205426  0.47815657 -0.07025039 -0.43120501]\n",
      " [        nan -0.03508568  0.27050732 -0.07722819 -0.24123132]\n",
      " ...\n",
      " [-0.51472908 -0.28083691         nan  0.9508242  -0.12583333]\n",
      " [-0.08194608 -0.21398956 -0.21351253         nan  0.59434767]\n",
      " [        nan -0.21667261 -0.17253563 -0.0603934   0.41214495]]\n",
      "training accuracy\n",
      "75.81875\n",
      "cost\n",
      "nan\n",
      "[[-0.38756191 -0.56866569         nan  0.9475828          nan]\n",
      " [        nan -0.04054222  0.44011619 -0.14057638 -0.52041789]\n",
      " [        nan         nan  0.22063967 -0.13030812 -0.24613796]\n",
      " ...\n",
      " [-0.51862168 -0.37718437         nan  0.94497285 -0.0136208 ]\n",
      " [-0.11078262 -0.31431923 -0.13781178         nan  0.56054994]\n",
      " [        nan -0.23761636 -0.14202657 -0.04617009  0.37508628]]\n",
      "training accuracy\n",
      "75.9875\n",
      "cost\n",
      "nan\n",
      "[[-0.37980466 -0.4378365          nan  0.75099576         nan]\n",
      " [        nan -0.05066792  0.44219974 -0.18115866 -0.55416232]\n",
      " [        nan         nan  0.17028522 -0.15471317 -0.12882545]\n",
      " ...\n",
      " [-0.56166357 -0.35484176         nan  0.96500922         nan]\n",
      " [-0.12843735 -0.37272425 -0.08601873         nan  0.55069704]\n",
      " [        nan -0.22873158 -0.09680752 -0.02569691  0.31319562]]\n",
      "training accuracy\n",
      "77.15625\n",
      "cost\n",
      "nan\n",
      "[[-0.48997286 -0.36440411 -0.03552471  0.72978021         nan]\n",
      " [        nan -0.04993277  0.39074711 -0.1916745  -0.52378251]\n",
      " [        nan         nan  0.13698458 -0.20056458 -0.03599243]\n",
      " ...\n",
      " [-0.69997006 -0.33876619 -0.04329987  1.12275317         nan]\n",
      " [-0.16976332 -0.4493049  -0.08764121         nan  0.59136161]\n",
      " [        nan -0.23481665 -0.1119414          nan  0.29282722]]\n",
      "training accuracy\n",
      "77.20625\n",
      "cost\n",
      "nan\n",
      "[[-0.6205287  -0.31420359 -0.1066598   0.74875692         nan]\n",
      " [        nan -0.06272798  0.31432571 -0.16897436 -0.38529567]\n",
      " [        nan -0.01028398  0.11342097 -0.17754966         nan]\n",
      " ...\n",
      " [-0.79194474 -0.30113465 -0.11379873  1.09013743         nan]\n",
      " [-0.21082828 -0.49465924 -0.11917948         nan  0.61575354]\n",
      " [-0.03620869 -0.23603238 -0.15777366         nan  0.30249786]]\n",
      "training accuracy\n",
      "77.3375\n",
      "cost\n",
      "nan\n",
      "[[-0.69726323 -0.24676876 -0.12092927  0.78890509         nan]\n",
      " [-0.00959599 -0.09889299  0.29977927 -0.18210505 -0.33680991]\n",
      " [-0.03401865 -0.04441409  0.10661698 -0.17987551         nan]\n",
      " ...\n",
      " [-0.7989273  -0.25352277 -0.12238965  0.98256746         nan]\n",
      " [-0.21178545 -0.51605918 -0.12575593         nan  0.60636227]\n",
      " [-0.05994693 -0.24281028 -0.1626768          nan  0.30469324]]\n",
      "training accuracy\n",
      "77.89375\n",
      "cost\n",
      "nan\n",
      "[[-0.74129083 -0.2227785  -0.12530778  0.78018778         nan]\n",
      " [-0.01392863 -0.10239195  0.30355009 -0.16640539 -0.38054049]\n",
      " [-0.04307113 -0.04695884  0.10407115 -0.16022236         nan]\n",
      " ...\n",
      " [-0.78371888 -0.24513905 -0.12587423  0.86604908         nan]\n",
      " [-0.19979031 -0.5611094  -0.12496585         nan  0.60982696]\n",
      " [-0.06697779 -0.24711166 -0.15987127         nan  0.29943052]]\n",
      "training accuracy\n",
      "78.26875\n",
      "cost\n",
      "nan\n",
      "[[-0.76637491 -0.21848896 -0.12731227  0.7545863          nan]\n",
      " [-0.0204779  -0.09128689  0.30399474 -0.14335038 -0.41649282]\n",
      " [-0.04996429 -0.03810559  0.10258565 -0.1388074          nan]\n",
      " ...\n",
      " [-0.76357028 -0.24884657 -0.13007961  0.77034717         nan]\n",
      " [-0.18691788 -0.61580357 -0.12207834         nan  0.61669168]\n",
      " [-0.07394948 -0.24698677 -0.15903953         nan  0.29566701]]\n",
      "training accuracy\n",
      "78.60625\n",
      "cost\n",
      "nan\n",
      "[[-0.78280859 -0.21623838 -0.12929242  0.72763112         nan]\n",
      " [-0.02878398 -0.08156634  0.30097121 -0.1236565  -0.43395449]\n",
      " [-0.05661591 -0.0320006   0.10158725 -0.12184245         nan]\n",
      " ...\n",
      " [-0.74629445 -0.25073113 -0.13582791  0.69778063         nan]\n",
      " [-0.17673001 -0.66591534 -0.1184067          nan  0.61972009]\n",
      " [-0.08104315 -0.24568631 -0.1602501          nan  0.29387543]]\n",
      "training accuracy\n",
      "78.9125\n",
      "cost\n",
      "nan\n",
      "[[-0.79446673 -0.2154167  -0.1326722   0.70276759         nan]\n",
      " [-0.03796234 -0.0726735   0.29602709 -0.10781399 -0.44016504]\n",
      " [-0.06387444 -0.02748002  0.10102809 -0.10849296         nan]\n",
      " ...\n",
      " [-0.73336479 -0.25121074 -0.14304267  0.64279508         nan]\n",
      " [-0.17010873 -0.71012327 -0.11399602         nan  0.61969665]\n",
      " [-0.08812247 -0.24388315 -0.16228116         nan  0.29326477]]\n",
      "training accuracy\n",
      "78.95625\n",
      "cost\n",
      "nan\n",
      "[[-0.80272932 -0.21571453 -0.13860927  0.68012066         nan]\n",
      " [-0.04810344 -0.06354093  0.29007144 -0.09561482 -0.4395909 ]\n",
      " [-0.07252311 -0.02335992  0.10091495 -0.09805855         nan]\n",
      " ...\n",
      " [-0.72376077 -0.25080481 -0.1513116   0.59931173         nan]\n",
      " [-0.16745163 -0.74848852 -0.10856369         nan  0.61725952]\n",
      " [-0.09549669 -0.24124429 -0.16428056         nan  0.29320035]]\n",
      "training accuracy\n",
      "79.0625\n",
      "cost\n",
      "nan\n",
      "[[-0.80798229 -0.21609952 -0.14887782  0.65904704         nan]\n",
      " [-0.05963786 -0.05390724  0.28358185 -0.08685536 -0.43424826]\n",
      " [-0.0831002  -0.0194138   0.10129842 -0.08985519         nan]\n",
      " ...\n",
      " [-0.71606847 -0.24933441 -0.16060271  0.5632456          nan]\n",
      " [-0.16974165 -0.78050874 -0.10184134         nan  0.61192597]\n",
      " [-0.10421211 -0.23720173 -0.16562261         nan  0.29306776]]\n",
      "training accuracy\n",
      "79.2\n",
      "cost\n",
      "nan\n",
      "[[-0.80935591 -0.21608222 -0.16600529  0.63823235         nan]\n",
      " [-0.07320911 -0.04292605  0.27696839 -0.08195649 -0.42633605]\n",
      " [-0.09612162 -0.01504276  0.10229122 -0.08344457         nan]\n",
      " ...\n",
      " [-0.70864273 -0.24698863 -0.17134575  0.5318035          nan]\n",
      " [-0.17900324 -0.80568693 -0.09331486         nan  0.60273962]\n",
      " [-0.11631458 -0.23106651 -0.16533666         nan  0.29178567]]\n",
      "training accuracy\n",
      "79.325\n",
      "cost\n",
      "nan\n",
      "[[-0.80468164 -0.21552436 -0.19260302  0.61581649         nan]\n",
      " [-0.09020886 -0.02896155  0.27046018 -0.08189026 -0.41855465]\n",
      " [-0.112776   -0.00912479  0.10401909 -0.07837987         nan]\n",
      " ...\n",
      " [-0.69969647 -0.24425197 -0.18433436  0.50255876         nan]\n",
      " [-0.19837536 -0.82364255 -0.08257417         nan  0.58933658]\n",
      " [-0.1353219  -0.22252926 -0.1618425          nan  0.28762375]]\n",
      "training accuracy\n",
      "79.60625\n",
      "cost\n",
      "nan\n",
      "[[-0.79109703 -0.21305687 -0.22845033  0.58919527         nan]\n",
      " [-0.11290393 -0.01109503  0.26403736 -0.08526139 -0.41369009]\n",
      " [-0.13483222 -0.0012878   0.10647283 -0.07245631         nan]\n",
      " ...\n",
      " [-0.68722315 -0.24044032 -0.19916101  0.47272238         nan]\n",
      " [-0.23033454 -0.83468899 -0.07016733         nan  0.57377239]\n",
      " [-0.16510063 -0.21293096 -0.15344393         nan  0.27937475]]\n",
      "training accuracy\n",
      "79.75\n",
      "cost\n",
      "nan\n",
      "[[-0.76813718 -0.20417462 -0.26868092  0.5559766          nan]\n",
      " [-0.14195486         nan  0.25816216 -0.08252791 -0.41472273]\n",
      " [-0.16211332         nan  0.10945344 -0.05894178         nan]\n",
      " ...\n",
      " [-0.67075571 -0.23157595 -0.21274912  0.44026172         nan]\n",
      " [-0.27198954 -0.84196272 -0.05729203         nan  0.55985213]\n",
      " [-0.20468711 -0.20689095 -0.13926921         nan  0.2681946 ]]\n",
      "training accuracy\n",
      "80.16250000000001\n",
      "cost\n",
      "nan\n",
      "[[-0.74085909 -0.1831122  -0.30765915  0.51642863         nan]\n",
      " [-0.17173184         nan  0.25464427 -0.05652459 -0.42719966]\n",
      " [-0.18943596         nan  0.11282651 -0.02770941         nan]\n",
      " ...\n",
      " [-0.65361263 -0.2118275  -0.22246723  0.4057327          nan]\n",
      " [-0.31084637 -0.8537734  -0.04448845         nan  0.55153728]\n",
      " [-0.24156834 -0.2128154  -0.11944163         nan  0.25691057]]\n",
      "training accuracy\n",
      "80.40625\n",
      "cost\n",
      "nan\n",
      "[[-0.71646346 -0.14512199 -0.3387185   0.47328915         nan]\n",
      " [-0.19331853         nan  0.25521773         nan -0.45658902]\n",
      " [-0.20949323 -0.01424545  0.1165076          nan -0.02969037]\n",
      " ...\n",
      " [-0.63870791 -0.17584759 -0.2251727   0.37027566         nan]\n",
      " [-0.3303289  -0.87983477 -0.03233231         nan  0.55210097]\n",
      " [-0.25688346 -0.2414096  -0.09711173         nan  0.24871885]]\n",
      "training accuracy\n",
      "80.5875\n",
      "cost\n",
      "nan\n",
      "[[-0.71257394 -0.10699368 -0.35541424  0.44931405         nan]\n",
      " [-0.19933173 -0.02999256  0.25434338         nan -0.47555503]\n",
      " [-0.21591941 -0.04200366  0.11890136         nan -0.0588581 ]\n",
      " ...\n",
      " [-0.64036442 -0.13863785 -0.21880468  0.34943891         nan]\n",
      " [-0.32409802 -0.91034472 -0.02581878         nan  0.55991963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-0.24075078 -0.28199759 -0.0855338          nan  0.24791962]]\n",
      "training accuracy\n",
      "80.71875\n",
      "cost\n",
      "nan\n",
      "[[-0.73765617 -0.1081603  -0.37040988  0.46090389         nan]\n",
      " [-0.20020421 -0.02247319  0.249272           nan -0.48308785]\n",
      " [-0.21685817 -0.02546761  0.11895173         nan -0.0805506 ]\n",
      " ...\n",
      " [-0.67529962 -0.14846838 -0.21560615  0.36124406         nan]\n",
      " [-0.31639504 -0.93949999 -0.01929106         nan  0.56550829]\n",
      " [-0.22026704 -0.30269237 -0.08167598         nan  0.24948   ]]\n",
      "training accuracy\n",
      "80.51875\n",
      "cost\n",
      "nan\n",
      "[[-0.75791633 -0.13555496 -0.39421548  0.47030602         nan]\n",
      " [-0.21669567         nan  0.24929181         nan -0.52728886]\n",
      " [-0.22965638         nan  0.11802044         nan -0.11878112]\n",
      " ...\n",
      " [-0.71467664 -0.20489818 -0.21466977  0.37795002         nan]\n",
      " [-0.33273748 -0.99963815         nan         nan  0.56531777]\n",
      " [-0.22401455 -0.31012819 -0.04394659         nan  0.24155077]]\n",
      "training accuracy\n",
      "79.86875\n",
      "cost\n",
      "nan\n",
      "[[-0.7417002  -0.08028611 -0.43568687  0.44143381         nan]\n",
      " [-0.24162301 -0.00503559  0.24402944         nan -0.48114138]\n",
      " [-0.24911757 -0.01297964  0.11967616         nan -0.1135011 ]\n",
      " ...\n",
      " [-0.70578217 -0.13151207 -0.24364547  0.34590945         nan]\n",
      " [-0.34895102 -0.98968268         nan         nan  0.57562277]\n",
      " [-0.24004058 -0.29626574 -0.04995277         nan  0.24168688]]\n",
      "training accuracy\n",
      "80.61874999999999\n",
      "cost\n",
      "nan\n",
      "[[-0.70243868 -0.02705267 -0.41106876  0.40655987         nan]\n",
      " [-0.24383841 -0.12700823  0.24389542         nan -0.47445325]\n",
      " [-0.25351305 -0.11482108  0.12328843         nan -0.13004086]\n",
      " ...\n",
      " [-0.66096632 -0.04422783 -0.23814267  0.30616997         nan]\n",
      " [-0.3311602  -0.91872094 -0.04205034         nan  0.57117074]\n",
      " [-0.22129767 -0.28630382 -0.12326646         nan  0.25249324]]\n",
      "training accuracy\n",
      "81.55\n",
      "cost\n",
      "nan\n",
      "[[-0.6327998          nan -0.38852418  0.35478651         nan]\n",
      " [-0.24058387 -0.35631645  0.23128805         nan -0.24729051]\n",
      " [-0.26820192 -0.32227203  0.1260521          nan         nan]\n",
      " ...\n",
      " [-0.58036352         nan -0.25694618  0.23786478         nan]\n",
      " [-0.290027   -0.59125034 -0.14589607         nan  0.51299176]\n",
      " [-0.18234337 -0.08337096 -0.23643437         nan  0.24183124]]\n",
      "training accuracy\n",
      "82.75\n",
      "cost\n",
      "nan\n",
      "[[-0.45191885 -0.00370295 -0.41008408  0.307362           nan]\n",
      " [-0.21116799 -0.35662157  0.23011702         nan -0.1251809 ]\n",
      " [-0.24768663 -0.27611931  0.12915959         nan         nan]\n",
      " ...\n",
      " [-0.44098782         nan -0.24310206  0.21482097         nan]\n",
      " [-0.32934321 -0.60265098 -0.10371754         nan  0.53063711]\n",
      " [-0.2727795  -0.15346731 -0.15369321         nan  0.25019362]]\n",
      "training accuracy\n",
      "82.63125\n",
      "cost\n",
      "nan\n",
      "[[-0.66713371 -0.09871505 -0.40878743  0.45823653         nan]\n",
      " [-0.1685816  -0.22604369  0.22498222         nan -0.18484771]\n",
      " [-0.20873575 -0.13839083  0.13079457         nan -0.00303253]\n",
      " ...\n",
      " [-0.65895493 -0.05341923 -0.29423215  0.34929501         nan]\n",
      " [-0.34092415 -0.70987852 -0.0849079          nan  0.53095332]\n",
      " [-0.23062915 -0.16853229 -0.18399888         nan  0.25431396]]\n",
      "training accuracy\n",
      "82.22500000000001\n",
      "cost\n",
      "nan\n",
      "[[-0.58276609 -0.09517851 -0.36964687  0.36820618         nan]\n",
      " [-0.22624995 -0.16691601  0.23188541         nan -0.26203081]\n",
      " [-0.27508282 -0.10088705  0.13291596         nan -0.06274478]\n",
      " ...\n",
      " [-0.55969909 -0.04856719 -0.27112888  0.26195694         nan]\n",
      " [-0.35155656 -0.69435594 -0.06149483         nan  0.52787631]\n",
      " [-0.23007159 -0.12662323 -0.16203588         nan  0.24397814]]\n",
      "training accuracy\n",
      "82.78750000000001\n",
      "cost\n",
      "nan\n",
      "[[-0.3952929  -0.02059141 -0.3190753   0.24446046         nan]\n",
      " [-0.22331674 -0.41766229  0.19261655         nan         nan]\n",
      " [-0.31807209 -0.3429696   0.12512766         nan         nan]\n",
      " ...\n",
      " [-0.36321057         nan -0.24833367  0.14630323         nan]\n",
      " [-0.33049704 -0.30520832 -0.19127564         nan  0.47187894]\n",
      " [-0.22279974         nan -0.27054126         nan  0.22366575]]\n",
      "training accuracy\n",
      "82.975\n",
      "cost\n",
      "nan\n",
      "[[-0.61541188 -0.12802104 -0.33068773  0.41653969         nan]\n",
      " [-0.21444588 -0.16520168  0.23046209         nan -0.23600803]\n",
      " [-0.27229484 -0.06475181  0.13806906         nan -0.05632311]\n",
      " ...\n",
      " [-0.61001715 -0.12506232 -0.25143093  0.33084307         nan]\n",
      " [-0.45837323 -0.73409101 -0.03477385         nan  0.55674967]\n",
      " [-0.33325783 -0.18144833 -0.13902268         nan  0.26160075]]\n",
      "training accuracy\n",
      "82.49375\n",
      "cost\n",
      "nan\n",
      "[[-0.68871028 -0.22748204 -0.44785699  0.45635628         nan]\n",
      " [-0.21151002         nan  0.25536582 -0.03620735 -0.51613984]\n",
      " [-0.28332404         nan  0.13626203 -0.03224134 -0.21989876]\n",
      " ...\n",
      " [-0.71796612 -0.31212032 -0.29937972  0.39594351         nan]\n",
      " [-0.48198999 -0.90651111         nan         nan  0.53215502]\n",
      " [-0.30640431 -0.20576699         nan         nan  0.21127173]]\n",
      "training accuracy\n",
      "80.13749999999999\n",
      "cost\n",
      "nan\n",
      "[[-0.60653044 -0.07766363 -0.42606039  0.37494357         nan]\n",
      " [-0.26489694 -0.07363686  0.21131652         nan -0.26512008]\n",
      " [-0.35474733 -0.04012028  0.13860034         nan -0.08162292]\n",
      " ...\n",
      " [-0.62403653 -0.08245382 -0.34781832  0.28941937         nan]\n",
      " [-0.5345007  -0.80557042         nan         nan  0.59170212]\n",
      " [-0.36973341 -0.15386913 -0.04528594         nan  0.23892288]]\n",
      "training accuracy\n",
      "82.83749999999999\n",
      "cost\n",
      "nan\n",
      "[[-0.41886036         nan -0.30545573  0.24732845         nan]\n",
      " [-0.215279   -0.46106235  0.20102627         nan -0.09325061]\n",
      " [-0.34600415 -0.38446329  0.14353519         nan         nan]\n",
      " ...\n",
      " [-0.3906799          nan -0.31708023  0.16306189         nan]\n",
      " [-0.43491826 -0.45594162 -0.1388334          nan  0.5252128 ]\n",
      " [-0.28815367 -0.003182   -0.23743829         nan  0.23870639]]\n",
      "training accuracy\n",
      "83.275\n",
      "cost\n",
      "nan\n",
      "[[-0.39372195         nan -0.29630702  0.26147762         nan]\n",
      " [-0.15685906 -0.47861182  0.19890528         nan -0.03896943]\n",
      " [-0.3065315  -0.3283145   0.14486123         nan         nan]\n",
      " ...\n",
      " [-0.37816901         nan -0.27253158  0.1924303          nan]\n",
      " [-0.45645314 -0.5977313  -0.04241553         nan  0.54979543]\n",
      " [-0.32859342 -0.12747422 -0.13604217         nan  0.25600909]]\n",
      "training accuracy\n",
      "83.33749999999999\n",
      "cost\n",
      "nan\n",
      "[[-0.43263995 -0.00231781 -0.36912379  0.29001482         nan]\n",
      " [-0.14070005 -0.38903416  0.19925649         nan -0.10990947]\n",
      " [-0.30758454 -0.23099916  0.14814286         nan -0.00256283]\n",
      " ...\n",
      " [-0.41225994         nan -0.37086009  0.21619001         nan]\n",
      " [-0.45286252 -0.58792234 -0.04490757         nan  0.53696446]\n",
      " [-0.3208476  -0.09641958 -0.13822769         nan  0.24619707]]\n",
      "training accuracy\n",
      "83.5875\n",
      "cost\n",
      "nan\n",
      "[[-0.44916335 -0.11489606 -0.34590775  0.2976751          nan]\n",
      " [-0.20113143 -0.18332604  0.20418078         nan -0.16679829]\n",
      " [-0.37214409 -0.05140971  0.14960997         nan -0.03984581]\n",
      " ...\n",
      " [-0.43363097 -0.05298185 -0.36059625  0.22610324         nan]\n",
      " [-0.49713982 -0.52290876 -0.02962193         nan  0.52456378]\n",
      " [-0.36458764 -0.01194153 -0.11815162         nan  0.24015029]]\n",
      "training accuracy\n",
      "83.91875\n",
      "cost\n",
      "nan\n",
      "[[-0.51388266 -0.22296077 -0.35965161  0.33426862         nan]\n",
      " [-0.20437123         nan  0.19856083         nan -0.22919322]\n",
      " [-0.36006264         nan  0.14470754         nan -0.08116045]\n",
      " ...\n",
      " [-0.50272481 -0.14542185 -0.39149394  0.26261298         nan]\n",
      " [-0.50288184 -0.46971185 -0.04365266         nan  0.52494526]\n",
      " [-0.35999441         nan -0.12573356         nan  0.2361841 ]]\n",
      "training accuracy\n",
      "84.40625\n",
      "cost\n",
      "nan\n",
      "[[-0.57282792 -0.26546428 -0.38985923  0.39276659         nan]\n",
      " [-0.15845396         nan  0.18120514 -0.19662186 -0.2810098 ]\n",
      " [-0.30273628         nan  0.13073116 -0.17673535 -0.10418312]\n",
      " ...\n",
      " [-0.62003703 -0.26900633 -0.37497326  0.34051667         nan]\n",
      " [-0.53210745 -0.60790798         nan         nan  0.54757601]\n",
      " [-0.37190577         nan -0.04624494 -0.02017022  0.23198477]]\n",
      "training accuracy\n",
      "83.71875\n",
      "cost\n",
      "nan\n",
      "[[-0.53077918 -0.29504625 -0.48810385  0.45383674         nan]\n",
      " [-0.23194682         nan  0.18554007 -0.44638766 -0.08805109]\n",
      " [-0.40001258         nan  0.13598037 -0.35204195         nan]\n",
      " ...\n",
      " [-0.67375823 -0.2948283  -0.34885481  0.34748713         nan]\n",
      " [-0.67759858 -0.74258678         nan -0.05855897  0.6390419 ]\n",
      " [-0.52714092 -0.01614383         nan -0.21478629  0.22768823]]\n",
      "training accuracy\n",
      "83.375\n",
      "cost\n",
      "nan\n",
      "[[-0.44038355 -0.03736165 -0.34172805  0.33802779         nan]\n",
      " [-0.18439939 -0.17900071  0.17817874 -0.37770141 -0.11668112]\n",
      " [-0.37292488 -0.14806969  0.15600125 -0.24676948 -0.01142837]\n",
      " ...\n",
      " [-0.44678641         nan -0.46017035  0.22137643         nan]\n",
      " [-0.49556235 -0.42024912 -0.18999298 -0.1524764   0.54943014]\n",
      " [-0.35918427         nan -0.28342102 -0.29739047  0.24684876]]\n",
      "training accuracy\n",
      "85.00625000000001\n",
      "cost\n",
      "nan\n",
      "[[-0.35560036 -0.03821689 -0.38407805  0.32071053         nan]\n",
      " [-0.18340485 -0.28943789  0.18652728 -0.33099954 -0.05071047]\n",
      " [-0.38179611 -0.18994812  0.15280809 -0.20568016         nan]\n",
      " ...\n",
      " [-0.40143151         nan -0.35975238  0.20705656         nan]\n",
      " [-0.49774051 -0.48894131 -0.05975295 -0.15735701  0.53814667]\n",
      " [-0.38145429 -0.00217022 -0.08786049 -0.30329028  0.23643313]]\n",
      "training accuracy\n",
      "84.63125000000001\n",
      "cost\n",
      "nan\n",
      "[[-0.44011343 -0.1007021  -0.35905914  0.37494259         nan]\n",
      " [-0.15482604 -0.17922282  0.16881223 -0.46871016         nan]\n",
      " [-0.35067933 -0.07238823  0.14738034 -0.33936341         nan]\n",
      " ...\n",
      " [-0.48474572 -0.03090589 -0.39824669  0.26213119         nan]\n",
      " [-0.4992911  -0.49947655 -0.06949148 -0.15223539  0.53471283]\n",
      " [-0.38187115         nan -0.13570799 -0.31502382  0.24529644]]\n",
      "training accuracy\n",
      "85.09375\n",
      "cost\n",
      "nan\n",
      "[[-0.4517791  -0.21864219 -0.37496744  0.3811448          nan]\n",
      " [-0.11477153         nan  0.17404817 -0.55974806 -0.12083568]\n",
      " [-0.31306658         nan  0.15025457 -0.4003462  -0.02914816]\n",
      " ...\n",
      " [-0.49568729 -0.10557348 -0.45542865  0.27371137         nan]\n",
      " [-0.4477836  -0.37768503 -0.13429305 -0.18290014  0.52212246]\n",
      " [-0.32387776         nan -0.20188901 -0.35618925  0.23557306]]\n",
      "training accuracy\n",
      "85.1625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost\n",
      "nan\n",
      "[[-0.41121682 -0.21362984 -0.36350916  0.3393787          nan]\n",
      " [-0.0902612          nan  0.17555446 -0.44545241 -0.11878048]\n",
      " [-0.29843904         nan  0.14886309 -0.29003222 -0.01183641]\n",
      " ...\n",
      " [-0.4401047  -0.05573008 -0.44117182  0.23309211         nan]\n",
      " [-0.37330088 -0.30015584 -0.17289486 -0.17792826  0.50786698]\n",
      " [-0.24799476         nan -0.22219469 -0.33271134  0.22795346]]\n",
      "training accuracy\n",
      "85.40625\n",
      "cost\n",
      "nan\n",
      "[[-0.31447423 -0.1604887  -0.3778992   0.30158351         nan]\n",
      " [-0.1296431  -0.12562393  0.15499679 -0.27561348         nan]\n",
      " [-0.35299356 -0.0486174   0.14139224 -0.15387715         nan]\n",
      " ...\n",
      " [-0.34245034         nan -0.3930391   0.19013803         nan]\n",
      " [-0.37300789 -0.27749693 -0.16049915 -0.16649347  0.51035003]\n",
      " [-0.27132415         nan -0.19087567 -0.30037639  0.24101835]]\n",
      "training accuracy\n",
      "85.58125\n",
      "cost\n",
      "nan\n",
      "[[-0.31576571 -0.26323464 -0.39712377  0.34160002         nan]\n",
      " [-0.17212791         nan  0.16766461 -0.35408008         nan]\n",
      " [-0.38542019         nan  0.15669152 -0.24174804         nan]\n",
      " ...\n",
      " [-0.38069227 -0.11802667 -0.43978841  0.23473349         nan]\n",
      " [-0.45432507 -0.34027078 -0.10770942 -0.14200465  0.52362499]\n",
      " [-0.36727099         nan -0.15724423 -0.28619698  0.24276975]]\n",
      "training accuracy\n",
      "85.44375\n",
      "cost\n",
      "nan\n",
      "[[-0.45615529 -0.2520579  -0.33270183  0.37116088         nan]\n",
      " [-0.07820372         nan  0.18539732 -0.59217903 -0.3481047 ]\n",
      " [-0.26638157         nan  0.15497444 -0.43761318 -0.21104885]\n",
      " ...\n",
      " [-0.5575359  -0.14973344 -0.42798825  0.2875122          nan]\n",
      " [-0.42781142 -0.39077218 -0.14617644 -0.16026145  0.50794767]\n",
      " [-0.30513132         nan -0.21942836 -0.32488406  0.22413238]]\n",
      "training accuracy\n",
      "84.75625\n",
      "cost\n",
      "nan\n",
      "[[-0.33937404 -0.24159959 -0.39038478  0.31781756         nan]\n",
      " [-0.10684487         nan  0.20585949 -0.40293309 -0.45267762]\n",
      " [-0.33515444         nan  0.15745558 -0.26059155 -0.22847825]\n",
      " ...\n",
      " [-0.4650616  -0.06676989 -0.41788261  0.20965827         nan]\n",
      " [-0.38181128 -0.29684171 -0.1333738  -0.19823721  0.46068047]\n",
      " [-0.26600177         nan -0.15467295 -0.34709128  0.19563822]]\n",
      "training accuracy\n",
      "84.675\n",
      "cost\n",
      "nan\n",
      "[[-0.40356775 -0.23023567 -0.43971097  0.33842368         nan]\n",
      " [-0.11974132         nan  0.16076106 -0.37222243 -0.01136468]\n",
      " [-0.34534784         nan  0.14246968 -0.25655673         nan]\n",
      " ...\n",
      " [-0.48439255 -0.10451643 -0.41932425  0.24369006         nan]\n",
      " [-0.3943045  -0.40815848 -0.07242031 -0.11827473  0.53095847]\n",
      " [-0.29146545         nan -0.12599194 -0.25467011  0.23875491]]\n",
      "training accuracy\n",
      "85.5875\n",
      "cost\n",
      "nan\n",
      "[[-0.29169215 -0.23615889 -0.35487646  0.34642291         nan]\n",
      " [-0.05976576         nan  0.12116575 -0.39817786         nan]\n",
      " [-0.28243709         nan  0.12311032 -0.29016439         nan]\n",
      " ...\n",
      " [-0.37666533 -0.08008665 -0.37432689  0.23430136         nan]\n",
      " [-0.34931623 -0.31956371 -0.13086781 -0.189163    0.51270431]\n",
      " [-0.27081394         nan -0.21734875 -0.34374675  0.2584967 ]]\n",
      "training accuracy\n",
      "85.56875000000001\n",
      "cost\n",
      "nan\n",
      "[[-0.33843168 -0.28699948 -0.41190294  0.35886789         nan]\n",
      " [-0.09023936         nan  0.17542712 -0.53120972 -0.13670793]\n",
      " [-0.32370446         nan  0.17191847 -0.38994028 -0.09307512]\n",
      " ...\n",
      " [-0.40329414 -0.11417298 -0.55724936  0.2505987          nan]\n",
      " [-0.33569272 -0.27734309 -0.24559813 -0.21989657  0.5244544 ]\n",
      " [-0.25221159         nan -0.32919381 -0.38059638  0.23819645]]\n",
      "training accuracy\n",
      "85.16875\n",
      "cost\n",
      "nan\n",
      "[[-0.32525509 -0.22986347 -0.36368385  0.31921774         nan]\n",
      " [-0.11095377         nan  0.20501017 -0.45281996 -0.40739416]\n",
      " [-0.34990498         nan  0.17046407 -0.30731536 -0.23100008]\n",
      " ...\n",
      " [-0.45160063 -0.08108846 -0.38335623  0.21472002         nan]\n",
      " [-0.37103906 -0.34637122 -0.12161833 -0.20565736  0.47514931]\n",
      " [-0.27780164         nan -0.15624032 -0.35360168  0.20255526]]\n",
      "training accuracy\n",
      "84.7375\n",
      "cost\n",
      "nan\n",
      "[[-0.39101955 -0.2545406  -0.46024996  0.33956278         nan]\n",
      " [-0.12178015         nan  0.17819636 -0.43631511 -0.20989248]\n",
      " [-0.3575724          nan  0.15429242 -0.30906536 -0.07427177]\n",
      " ...\n",
      " [-0.5217669  -0.12737246 -0.43925717  0.24632946         nan]\n",
      " [-0.39752966 -0.42742391 -0.05537255 -0.14045706  0.51136647]\n",
      " [-0.30583431         nan -0.10946489 -0.28294565  0.2208135 ]]\n",
      "training accuracy\n",
      "85.1125\n",
      "cost\n",
      "nan\n",
      "[[-0.33640265 -0.22567645 -0.3858696   0.34615193         nan]\n",
      " [-0.09792137         nan  0.13304378 -0.46554669         nan]\n",
      " [-0.3341875          nan  0.13515922 -0.34850836         nan]\n",
      " ...\n",
      " [-0.42311174 -0.1214202  -0.36465709  0.2542837          nan]\n",
      " [-0.35506738 -0.45047714 -0.05620981 -0.14401707  0.51474585]\n",
      " [-0.28027168         nan -0.16115838 -0.29289446  0.2484356 ]]\n",
      "training accuracy\n",
      "85.98125\n",
      "cost\n",
      "nan\n",
      "[[-0.31922563 -0.3522302  -0.40232289  0.38839941         nan]\n",
      " [-0.02245625         nan  0.1225661  -0.54707636         nan]\n",
      " [-0.24738887         nan  0.12991101 -0.4263009          nan]\n",
      " ...\n",
      " [-0.44825592 -0.2376709  -0.41254702  0.28604113         nan]\n",
      " [-0.34076997 -0.42380449 -0.05387133 -0.20195042  0.53576938]\n",
      " [-0.27547251         nan -0.17198646 -0.38107329  0.25104909]]\n",
      "training accuracy\n",
      "85.9875\n",
      "cost\n",
      "nan\n",
      "[[-0.25027143 -0.19966601 -0.37860734  0.30852406         nan]\n",
      " [-0.15366107         nan  0.19196293 -0.45489905 -0.25155565]\n",
      " [-0.423348           nan  0.19127152 -0.31957114 -0.19431955]\n",
      " ...\n",
      " [-0.31937079 -0.00594316 -0.57347202  0.19904092         nan]\n",
      " [-0.28956889 -0.24613014 -0.32226844 -0.25127664  0.5015953 ]\n",
      " [-0.22542956         nan -0.44603685 -0.39335536  0.23125395]]\n",
      "training accuracy\n",
      "84.5\n",
      "cost\n",
      "nan\n",
      "[[-0.33072722 -0.32870814 -0.41309522  0.33682672         nan]\n",
      " [-0.13373907         nan  0.20231711 -0.47104855 -0.42074124]\n",
      " [-0.38697741         nan  0.17269126 -0.33681988 -0.23287338]\n",
      " ...\n",
      " [-0.51352531 -0.16831143 -0.37503101  0.23675745         nan]\n",
      " [-0.4017518  -0.38282437 -0.05343313 -0.18470213  0.47625982]\n",
      " [-0.33135609         nan -0.10486972 -0.33002123  0.20072494]]\n",
      "training accuracy\n",
      "84.7125\n",
      "cost\n",
      "nan\n",
      "[[-0.35638778 -0.24417674 -0.45744285  0.32670818         nan]\n",
      " [-0.14404776         nan  0.17106048 -0.42009059 -0.10486229]\n",
      " [-0.39963734         nan  0.16026308 -0.30195257 -0.00706936]\n",
      " ...\n",
      " [-0.47455111 -0.11664567 -0.4428664   0.23774049         nan]\n",
      " [-0.36223446 -0.46162576 -0.05070869 -0.13012602  0.51301843]\n",
      " [-0.29408702         nan -0.15119177 -0.26429303  0.23008605]]\n",
      "training accuracy\n",
      "85.5125\n",
      "cost\n",
      "nan\n",
      "[[-0.27616293 -0.24020149 -0.38613599  0.32727292         nan]\n",
      " [-0.14099939         nan  0.13193775 -0.44395741         nan]\n",
      " [-0.39326365         nan  0.13804668 -0.33192077         nan]\n",
      " ...\n",
      " [-0.36981153 -0.11910242 -0.35257903  0.23198125         nan]\n",
      " [-0.34390657 -0.45426083 -0.02875417 -0.16106254  0.49878715]\n",
      " [-0.29124192         nan -0.17071918 -0.31022672  0.24778314]]\n",
      "training accuracy\n",
      "85.69375000000001\n",
      "cost\n",
      "nan\n",
      "[[-0.30325035 -0.37507957 -0.39687579  0.37616715         nan]\n",
      " [-0.04465993         nan  0.12090663 -0.55444947         nan]\n",
      " [-0.2783997          nan  0.12969062 -0.43662124         nan]\n",
      " ...\n",
      " [-0.45755928 -0.27820757 -0.35525294  0.28352621         nan]\n",
      " [-0.35255781 -0.51927255         nan -0.18193386  0.53059895]\n",
      " [-0.30649655         nan -0.11148377 -0.363305    0.24783956]]\n",
      "training accuracy\n",
      "85.95625\n",
      "cost\n",
      "nan\n",
      "[[-0.24852733 -0.21936577 -0.47518615  0.32592245         nan]\n",
      " [-0.1873857          nan  0.18150638 -0.50021755 -0.16967187]\n",
      " [-0.46485069         nan  0.18813756 -0.37317398 -0.13846635]\n",
      " ...\n",
      " [-0.35094155 -0.06378461 -0.67253949  0.22442617         nan]\n",
      " [-0.33129282 -0.38217273 -0.24175491 -0.21340778  0.53211847]\n",
      " [-0.28587313         nan -0.42268785 -0.35733758  0.24320252]]\n",
      "training accuracy\n",
      "84.6625\n",
      "cost\n",
      "nan\n",
      "[[-0.24194789 -0.30753729 -0.38035194  0.31026356         nan]\n",
      " [-0.16753819         nan  0.20311902 -0.42003017 -0.44460493]\n",
      " [-0.44851133         nan  0.18265416 -0.29658046 -0.26190517]\n",
      " ...\n",
      " [-0.44865645 -0.14236259 -0.33828436  0.21547362         nan]\n",
      " [-0.39607711 -0.4109652  -0.04490063 -0.18111022  0.45848274]\n",
      " [-0.34940061         nan -0.13585153 -0.318043    0.19738305]]\n",
      "training accuracy\n",
      "84.61874999999999\n",
      "cost\n",
      "nan\n",
      "[[-0.34045272 -0.29104634 -0.45708458  0.31942606         nan]\n",
      " [-0.15646246         nan  0.18219029 -0.42214544 -0.24077869]\n",
      " [-0.42990421         nan  0.16991771 -0.30687838 -0.10908099]\n",
      " ...\n",
      " [-0.48929369 -0.13710337 -0.44336908  0.23477538         nan]\n",
      " [-0.35733667 -0.47678842 -0.02610676 -0.12711205  0.49279088]\n",
      " [-0.30415715         nan -0.15642296 -0.26148141  0.2204492 ]]\n",
      "training accuracy\n",
      "85.18124999999999\n",
      "cost\n",
      "nan\n",
      "[[-0.2990238  -0.23731181 -0.41765696  0.30890419         nan]\n",
      " [-0.2166954          nan  0.16058393 -0.41106813         nan]\n",
      " [-0.49193362         nan  0.16230238 -0.30071597         nan]\n",
      " ...\n",
      " [-0.37888989 -0.10230154 -0.41474451  0.22066379         nan]\n",
      " [-0.33594237 -0.49246555 -0.0194554  -0.13106104  0.48887286]\n",
      " [-0.28724009         nan -0.18911397 -0.26687293  0.23656142]]\n",
      "training accuracy\n",
      "86.24374999999999\n",
      "cost\n",
      "nan\n",
      "[[-0.24537442 -0.35542822 -0.32711809  0.34294916         nan]\n",
      " [-0.09013974         nan  0.11323236 -0.49914218         nan]\n",
      " [-0.33573481         nan  0.12171568 -0.38867596         nan]\n",
      " ...\n",
      " [-0.41704054 -0.25301313 -0.18671443  0.25181204         nan]\n",
      " [-0.35973347 -0.57115611         nan -0.1685182   0.48999513]\n",
      " [-0.33616266         nan -0.02997927 -0.34994664  0.25038822]]\n",
      "training accuracy\n",
      "85.60624999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost\n",
      "nan\n",
      "[[-0.31610401 -0.43660717 -0.09895434  0.3481127          nan]\n",
      " [-0.03479538         nan  0.10814276 -0.4796052          nan]\n",
      " [-0.26618042         nan  0.11442664 -0.38464107         nan]\n",
      " ...\n",
      " [-0.56729493 -0.34137718         nan  0.26434802         nan]\n",
      " [-0.37157361 -0.6755989          nan -0.16627366  0.51159607]\n",
      " [-0.34879708         nan         nan -0.37523849  0.24865707]]\n",
      "training accuracy\n",
      "85.63125000000001\n",
      "cost\n",
      "nan\n",
      "[[-0.30939221 -0.28204306         nan  0.28145306 -0.29506457]\n",
      " [-0.08604252         nan  0.13839905 -0.33492621         nan]\n",
      " [-0.35932357         nan  0.13380516 -0.26628181         nan]\n",
      " ...\n",
      " [-0.51867143 -0.18098467         nan  0.21055879 -0.17663651]\n",
      " [-0.33593208 -0.64001479         nan -0.15404818  0.47127914]\n",
      " [-0.31662701         nan -0.02497685 -0.33536776  0.27433769]]\n",
      "training accuracy\n",
      "85.38125\n",
      "cost\n",
      "nan\n",
      "[[-0.23934674 -0.21657775         nan  0.25565498 -0.27112747]\n",
      " [-0.20025123         nan  0.13498536 -0.36307658         nan]\n",
      " [-0.51167906         nan  0.13076426 -0.28506071         nan]\n",
      " ...\n",
      " [-0.43139582 -0.12804967         nan  0.19736672 -0.14793502]\n",
      " [-0.38655952 -0.7050412          nan -0.12098281  0.42765079]\n",
      " [-0.37427299         nan -0.05697446 -0.28889144  0.23694037]]\n",
      "training accuracy\n",
      "85.94375\n",
      "cost\n",
      "nan\n",
      "[[-0.25733777 -0.15999911         nan  0.25497787 -0.24646007]\n",
      " [-0.1580238  -0.0504181   0.12141576 -0.41226244         nan]\n",
      " [-0.46326274         nan  0.12085794 -0.32683966         nan]\n",
      " ...\n",
      " [-0.43576403 -0.09885212         nan  0.20242995 -0.17415636]\n",
      " [-0.34197654 -0.78336374         nan -0.11607295  0.41556472]\n",
      " [-0.33044123         nan -0.03436389 -0.29338697  0.23734572]]\n",
      "training accuracy\n",
      "85.66875\n",
      "cost\n",
      "nan\n",
      "[[-0.22483409 -0.07145152         nan  0.26062261 -0.05675068]\n",
      " [-0.09748964 -0.26307319  0.1141074  -0.33358445         nan]\n",
      " [-0.41259995 -0.11234062  0.1147932  -0.26852981         nan]\n",
      " ...\n",
      " [-0.47499217 -0.00752691         nan  0.19355313 -0.02830704]\n",
      " [-0.3576069  -0.87430893         nan -0.1559536   0.53528259]\n",
      " [-0.35577691 -0.06026467 -0.07779275 -0.34889109  0.29669268]]\n",
      "training accuracy\n",
      "85.1\n",
      "cost\n",
      "nan\n",
      "[[-0.21620004         nan         nan  0.21614322 -0.23807352]\n",
      " [        nan -0.71467386  0.12135179 -0.31142225         nan]\n",
      " [-0.2754799  -0.54540518  0.11035794 -0.25040095         nan]\n",
      " ...\n",
      " [-0.49170264         nan         nan  0.16662549 -0.14154265]\n",
      " [-0.27961621 -1.28431592         nan -0.14854036  0.49376156]\n",
      " [-0.27300738 -0.49586906 -0.16277076 -0.32791709  0.29035481]]\n",
      "training accuracy\n",
      "84.1375\n",
      "cost\n",
      "nan\n",
      "[[-1.74801257e-01             nan             nan  1.76944070e-01\n",
      "  -1.45461629e-01]\n",
      " [-1.05703755e-01 -6.95881509e-01  1.40608328e-01 -4.30206969e-02\n",
      "              nan]\n",
      " [-4.68513239e-01 -5.02676825e-01  1.31431911e-01 -6.29697325e-04\n",
      "              nan]\n",
      " ...\n",
      " [-3.86387107e-01             nan             nan  1.40130522e-01\n",
      "  -7.91570990e-02]\n",
      " [-3.08919589e-01 -1.23678132e+00             nan             nan\n",
      "   4.59200003e-01]\n",
      " [-2.86743320e-01 -4.24595061e-01 -4.09721066e-02 -1.19572533e-01\n",
      "   2.56264164e-01]]\n",
      "training accuracy\n",
      "83.625\n",
      "cost\n",
      "nan\n",
      "[[-0.14027926         nan         nan  0.15387809 -0.01159353]\n",
      " [-0.06554674 -0.69422824  0.13349101         nan         nan]\n",
      " [-0.4243732  -0.51174154  0.12380463         nan         nan]\n",
      " ...\n",
      " [-0.44700966         nan         nan  0.1460214          nan]\n",
      " [-0.35875568 -1.34713992         nan         nan  0.52951323]\n",
      " [-0.35900792 -0.54116365 -0.04421968         nan  0.28799633]]\n",
      "training accuracy\n",
      "83.39375\n",
      "cost\n",
      "nan\n",
      "[[-0.19116057         nan         nan  0.16884295 -0.13796346]\n",
      " [        nan -0.63179211  0.14364748         nan         nan]\n",
      " [-0.32946736 -0.45182299  0.12800196         nan         nan]\n",
      " ...\n",
      " [-0.46417208         nan         nan  0.15078762 -0.06208517]\n",
      " [-0.24984173 -1.25809906         nan         nan  0.47353115]\n",
      " [-0.23350583 -0.45559725 -0.09037741         nan  0.271733  ]]\n",
      "training accuracy\n",
      "83.59375\n",
      "cost\n",
      "nan\n",
      "[[-0.15975954         nan         nan  0.16592257 -0.24383044]\n",
      " [-0.06749242 -0.63990404  0.15991042         nan         nan]\n",
      " [-0.43545353 -0.44364648  0.13960244         nan         nan]\n",
      " ...\n",
      " [-0.40133405         nan         nan  0.1401188  -0.10758349]\n",
      " [-0.26015023 -1.21422638         nan         nan  0.42462959]\n",
      " [-0.24792425 -0.39242587 -0.10165069         nan  0.25236365]]\n",
      "training accuracy\n",
      "84.0375\n",
      "cost\n",
      "nan\n",
      "[[-0.1371208          nan         nan  0.16445694 -0.28246686]\n",
      " [-0.11099758 -0.649039    0.16118346         nan         nan]\n",
      " [-0.48945822 -0.4389909   0.14003262         nan         nan]\n",
      " ...\n",
      " [-0.38786894         nan         nan  0.13734449 -0.09213126]\n",
      " [-0.27868627 -1.20629947         nan         nan  0.39094902]\n",
      " [-0.28285942 -0.36531528 -0.09721583         nan  0.23544054]]\n",
      "training accuracy\n",
      "84.325\n",
      "cost\n",
      "nan\n",
      "[[-0.12406565         nan         nan  0.1652405  -0.27123334]\n",
      " [-0.10189503 -0.66094357  0.15540724         nan -0.05224159]\n",
      " [-0.47782091 -0.44311731  0.13634548         nan         nan]\n",
      " ...\n",
      " [-0.38554715         nan         nan  0.14061093 -0.06190955]\n",
      " [-0.27168828 -1.22861778         nan         nan  0.37446737]\n",
      " [-0.29228273 -0.37126632 -0.07243895         nan  0.22415592]]\n",
      "training accuracy\n",
      "84.46249999999999\n",
      "cost\n",
      "nan\n",
      "[[-0.13747036         nan         nan  0.16911942 -0.25444302]\n",
      " [-0.07319516 -0.67567054  0.14792623         nan         nan]\n",
      " [-0.44351038 -0.44931131  0.13331193         nan         nan]\n",
      " ...\n",
      " [-0.391767           nan         nan  0.14364708 -0.07995315]\n",
      " [-0.2509923  -1.24958235         nan         nan  0.37106367]\n",
      " [-0.2730119  -0.38694303 -0.02893931         nan  0.22526762]]\n",
      "training accuracy\n",
      "84.52499999999999\n",
      "cost\n",
      "nan\n",
      "[[-0.15564245         nan         nan  0.17568032 -0.17397082]\n",
      " [-0.0753938  -0.66791866  0.14429953         nan         nan]\n",
      " [-0.44906652 -0.43747738  0.13309192         nan         nan]\n",
      " ...\n",
      " [-0.41054921         nan         nan  0.15016076 -0.08972   ]\n",
      " [-0.25475311 -1.25925514         nan         nan  0.3978135 ]\n",
      " [-0.26289156 -0.39857174 -0.01938133         nan  0.24255141]]\n",
      "training accuracy\n",
      "84.16875\n",
      "cost\n",
      "nan\n",
      "[[-0.1763362          nan         nan  0.18136299 -0.03609309]\n",
      " [-0.04649653 -0.67444207  0.13901254         nan         nan]\n",
      " [-0.41725001 -0.45396914  0.12610371         nan         nan]\n",
      " ...\n",
      " [-0.4969545          nan         nan  0.16320491         nan]\n",
      " [-0.29149845 -1.31243033         nan         nan  0.46936154]\n",
      " [-0.28825735 -0.47307227 -0.07317519         nan  0.27064408]]\n",
      "training accuracy\n",
      "84.2\n",
      "cost\n",
      "nan\n",
      "[[-0.18483037         nan         nan  0.1829236  -0.08444658]\n",
      " [-0.00726336 -0.68907833  0.14280952         nan         nan]\n",
      " [-0.37668198 -0.46126396  0.1235031          nan         nan]\n",
      " ...\n",
      " [-0.50060845         nan         nan  0.16290535 -0.02312125]\n",
      " [-0.26977778 -1.30004257         nan         nan  0.45873093]\n",
      " [-0.24809221 -0.47840931 -0.11409375         nan  0.26477077]]\n",
      "training accuracy\n",
      "84.28125\n",
      "cost\n",
      "nan\n",
      "[[-0.16295338         nan         nan  0.17969132 -0.14397858]\n",
      " [-0.05395352 -0.68992416  0.1533853          nan         nan]\n",
      " [-0.43931706 -0.44654301  0.12798545         nan         nan]\n",
      " ...\n",
      " [-0.46809459         nan         nan  0.1584571  -0.06292433]\n",
      " [-0.29247407 -1.27036152         nan         nan  0.43554166]\n",
      " [-0.26434073 -0.44315808 -0.12333295         nan  0.25745821]]\n",
      "training accuracy\n",
      "84.325\n",
      "cost\n",
      "nan\n",
      "[[-0.15819538         nan         nan  0.17829529 -0.17043782]\n",
      " [-0.08043032 -0.70064863  0.15824545         nan         nan]\n",
      " [-0.4754062  -0.43626133  0.12873869         nan         nan]\n",
      " ...\n",
      " [-0.4639252          nan         nan  0.1558428  -0.0792516 ]\n",
      " [-0.31044304 -1.24737249         nan         nan  0.41690023]\n",
      " [-0.27777036 -0.41805649 -0.12970216         nan  0.25185159]]\n",
      "training accuracy\n",
      "84.38125\n",
      "cost\n",
      "nan\n",
      "[[-0.15595652         nan         nan  0.17817508 -0.17837688]\n",
      " [-0.08595728 -0.71634439  0.16123826         nan         nan]\n",
      " [-0.48330545 -0.43271574  0.12832718         nan         nan]\n",
      " ...\n",
      " [-0.46129056         nan         nan  0.15486823 -0.08268416]\n",
      " [-0.31082451 -1.23332885         nan         nan  0.40476568]\n",
      " [-0.27372797 -0.40531313 -0.13523192         nan  0.24798032]]\n",
      "training accuracy\n",
      "84.46249999999999\n",
      "cost\n",
      "nan\n",
      "[[-0.15355514         nan         nan  0.17811735 -0.17263848]\n",
      " [-0.08879923 -0.73253796  0.16287329         nan         nan]\n",
      " [-0.48692358 -0.43265337  0.12697993         nan         nan]\n",
      " ...\n",
      " [-0.46400705         nan         nan  0.15521687 -0.07911883]\n",
      " [-0.31406144 -1.2264294          nan         nan  0.39944787]\n",
      " [-0.27272239 -0.40268972 -0.13880422         nan  0.24598249]]\n",
      "training accuracy\n",
      "84.5125\n",
      "cost\n",
      "nan\n",
      "[[-0.15271556         nan         nan  0.17829048 -0.15749846]\n",
      " [-0.08916293 -0.74777323  0.16395122         nan         nan]\n",
      " [-0.48754542 -0.43375199  0.12520112         nan         nan]\n",
      " ...\n",
      " [-0.47166384         nan         nan  0.15634815 -0.07059627]\n",
      " [-0.32097353 -1.22173729         nan         nan  0.39998208]\n",
      " [-0.27409895 -0.40540537 -0.14333211         nan  0.24569332]]\n",
      "training accuracy\n",
      "84.54374999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost\n",
      "nan\n",
      "[[-0.15178151         nan         nan  0.1783249  -0.14036201]\n",
      " [-0.08681139 -0.76348381  0.16495946         nan         nan]\n",
      " [-0.48558269 -0.43633923  0.12322431         nan         nan]\n",
      " ...\n",
      " [-0.48132891         nan         nan  0.1575589  -0.06019793]\n",
      " [-0.33039187 -1.21746344         nan         nan  0.40352708]\n",
      " [-0.27726456 -0.41121734 -0.14914583         nan  0.24599088]]\n",
      "training accuracy\n",
      "84.60625\n",
      "cost\n",
      "nan\n",
      "[[-0.14829629         nan         nan  0.17799813 -0.13239116]\n",
      " [-0.08494317 -0.7792786   0.16670784         nan         nan]\n",
      " [-0.4850991  -0.43876389  0.12155466         nan         nan]\n",
      " ...\n",
      " [-0.48738762         nan         nan  0.15820895 -0.05462285]\n",
      " [-0.34060851 -1.21050968         nan         nan  0.40500815]\n",
      " [-0.28180625 -0.41524787 -0.15440945         nan  0.24533012]]\n",
      "training accuracy\n",
      "84.65625\n",
      "cost\n",
      "nan\n",
      "[[-0.14134441         nan         nan  0.17732875 -0.13840227]\n",
      " [-0.08764171 -0.79357682  0.16963703         nan         nan]\n",
      " [-0.49050548 -0.43861808  0.12051875         nan         nan]\n",
      " ...\n",
      " [-0.48667819         nan         nan  0.15814919 -0.0577192 ]\n",
      " [-0.35097103 -1.1985562          nan         nan  0.40102375]\n",
      " [-0.28804103 -0.41320755 -0.15721735         nan  0.24298314]]\n",
      "training accuracy\n",
      "84.71875\n",
      "cost\n",
      "nan\n",
      "[[-0.13223401         nan         nan  0.17642927 -0.15275597]\n",
      " [-0.09507943 -0.80631663  0.17328194         nan         nan]\n",
      " [-0.50096812 -0.43572929  0.11984887         nan         nan]\n",
      " ...\n",
      " [-0.48117481         nan         nan  0.15761619 -0.06589057]\n",
      " [-0.36103419 -1.18224811         nan         nan  0.39248671]\n",
      " [-0.29580353 -0.40540705 -0.15718001         nan  0.23924564]]\n",
      "training accuracy\n",
      "84.8375\n",
      "cost\n",
      "nan\n",
      "[[-0.12261741         nan         nan  0.17544554 -0.16726369]\n",
      " [-0.1042095  -0.81864563  0.17684833         nan         nan]\n",
      " [-0.51180425 -0.43173909  0.11907274         nan         nan]\n",
      " ...\n",
      " [-0.47437846         nan         nan  0.15695781 -0.07367133]\n",
      " [-0.36984167 -1.16372237         nan         nan  0.38231802]\n",
      " [-0.30401948 -0.39508676 -0.1545277          nan  0.23481132]]\n",
      "training accuracy\n",
      "84.96875\n",
      "cost\n",
      "nan\n",
      "[[-0.11359724         nan         nan  0.17451781 -0.17676709]\n",
      " [-0.11253475 -0.83134934  0.17986703         nan         nan]\n",
      " [-0.51945919 -0.42824775  0.11793403         nan         nan]\n",
      " ...\n",
      " [-0.46888221         nan         nan  0.15645912 -0.07810151]\n",
      " [-0.37723791 -1.14467577         nan         nan  0.3731496 ]\n",
      " [-0.3121643  -0.38526991 -0.14955304         nan  0.23046244]]\n",
      "training accuracy\n",
      "85.0875\n",
      "cost\n",
      "nan\n",
      "[[-0.10615546         nan         nan  0.17374261 -0.17882446]\n",
      " [-0.11927694 -0.84464799  0.18224486         nan         nan]\n",
      " [-0.52298141 -0.42614444  0.1164037          nan         nan]\n",
      " ...\n",
      " [-0.46637216         nan         nan  0.15623453 -0.07823451]\n",
      " [-0.38412119 -1.12582815         nan         nan  0.36687913]\n",
      " [-0.32051916 -0.3775976  -0.14287796         nan  0.22693174]]\n",
      "training accuracy\n",
      "85.1375\n",
      "cost\n",
      "nan\n",
      "[[-0.10132133         nan         nan  0.1731563  -0.17225572]\n",
      " [-0.12451802 -0.85845778  0.18402052         nan         nan]\n",
      " [-0.5228061  -0.42569103  0.11453138         nan         nan]\n",
      " ...\n",
      " [-0.46801038         nan         nan  0.15622973 -0.07356959]\n",
      " [-0.39162971 -1.10732105         nan         nan  0.3648511 ]\n",
      " [-0.32956049 -0.37265866 -0.13550215         nan  0.22476952]]\n",
      "training accuracy\n",
      "85.1625\n",
      "cost\n",
      "nan\n",
      "[[-0.099656           nan         nan  0.17270617 -0.15718214]\n",
      " [-0.12828727 -0.87248726  0.18518918         nan         nan]\n",
      " [-0.51954275 -0.42669471  0.11235677         nan         nan]\n",
      " ...\n",
      " [-0.47417526         nan         nan  0.15625436 -0.0636069 ]\n",
      " [-0.40039514 -1.08922784         nan         nan  0.36762348]\n",
      " [-0.33959194 -0.37053626 -0.12831451         nan  0.22411062]]\n",
      "training accuracy\n",
      "85.26875000000001\n",
      "cost\n",
      "nan\n",
      "[[-0.10024764         nan         nan  0.17221845 -0.13714311]\n",
      " [-0.13033123 -0.88627356  0.18577065         nan         nan]\n",
      " [-0.51359188 -0.42860025  0.10994545         nan         nan]\n",
      " ...\n",
      " [-0.48315825         nan         nan  0.15601156 -0.04955842]\n",
      " [-0.40979869 -1.07155986         nan         nan  0.37387812]\n",
      " [-0.35032663 -0.37074334 -0.12138149         nan  0.22435779]]\n",
      "training accuracy\n",
      "85.38125\n",
      "cost\n",
      "nan\n",
      "[[-0.10014973         nan         nan  0.17144376 -0.12096081]\n",
      " [-0.13142655 -0.89889398  0.18620402         nan         nan]\n",
      " [-0.50627451 -0.4303697   0.1075388          nan         nan]\n",
      " ...\n",
      " [-0.49008074         nan         nan  0.15524336 -0.03701624]\n",
      " [-0.41786413 -1.05368008         nan         nan  0.37941896]\n",
      " [-0.36061114 -0.37148372 -0.11377236         nan  0.22422059]]\n",
      "training accuracy\n",
      "85.5125\n",
      "cost\n",
      "nan\n",
      "[[-0.09658687         nan         nan  0.17020267 -0.11722197]\n",
      " [-0.13457825 -0.90867024  0.18733105         nan         nan]\n",
      " [-0.50072346 -0.43048584  0.10548562         nan         nan]\n",
      " ...\n",
      " [-0.49035891         nan         nan  0.15395359 -0.0327572 ]\n",
      " [-0.42315969 -1.03431506         nan         nan  0.37948424]\n",
      " [-0.36949467 -0.36992892 -0.10448909         nan  0.22264074]]\n",
      "training accuracy\n",
      "85.56875000000001\n",
      "cost\n",
      "nan\n",
      "[[-0.08981607         nan         nan  0.16851374 -0.12536417]\n",
      " [-0.14238173 -0.91462786  0.18947684         nan         nan]\n",
      " [-0.49913528 -0.42810341  0.10384832         nan         nan]\n",
      " ...\n",
      " [-0.48452927         nan         nan  0.15234958 -0.03708712]\n",
      " [-0.42607931 -1.01296453         nan         nan  0.37309725]\n",
      " [-0.37713362 -0.36461888 -0.09345212         nan  0.21958019]]\n",
      "training accuracy\n",
      "85.64375\n",
      "cost\n",
      "nan\n",
      "[[-0.08191196         nan         nan  0.16658438 -0.13777211]\n",
      " [-0.15403472 -0.91778434  0.19213144         nan         nan]\n",
      " [-0.50015126 -0.42397372  0.1023599          nan         nan]\n",
      " ...\n",
      " [-0.47630544         nan         nan  0.15065566 -0.04472884]\n",
      " [-0.42727387 -0.99059001         nan         nan  0.363032  ]\n",
      " [-0.38375169 -0.35677965 -0.08119458         nan  0.21569047]]\n",
      "training accuracy\n",
      "85.64375\n",
      "validation accuracy\n",
      "86.5\n",
      "validation cost\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    #one epoch\n",
    "    for i in range(int(xtrain.shape[0]/batch)):#for each iteration, we use a for loop.\n",
    "        a0,z1,a1,z2,a2=forwardpass(xtrain[i*batch:(i+1)*batch,:])\n",
    "        y=a2\n",
    "        #print(y)\n",
    "        #print(y.shape)\n",
    "        labelBatch=ytrain[i*batch:(i+1)*batch,:]\n",
    "        #print(labelBatch)\n",
    "        #Backward pass\n",
    "        del2=(y-labelBatch)\n",
    "        #print(\"del2\",del2.shape, (w2.T).shape)\n",
    "        del1=np.dot(del2,w2.T)*(1-pow(actFunc(z1,3),2))\n",
    "        #print(\"del1\",del1.shape)\n",
    "        #print((a1.T).shape)\n",
    "        dcdw2=np.dot(a1.T,del2)\n",
    "        #print(\"dcdw2\",dcdw2.shape)\n",
    "        dcdw1=np.dot(a0.T,del1)\n",
    "        #print(\"dcdw1\",dcdw1.shape)\n",
    "        dcdb1=np.sum(del1,axis=0)\n",
    "        #print(\"dcdb1\",dcdb1.shape)\n",
    "        dcdb2=np.sum(del2,axis=0)\n",
    "        #print(\"dcdb2\",dcdb2.shape)\n",
    "        #print(w1.shape, w2.shape, b1.shape, b2.shape)\n",
    "        w1=w1-alpha*dcdw1\n",
    "        w2=w2-alpha*dcdw2\n",
    "        b2=b2-alpha*dcdb2\n",
    "        b1=b1-alpha*dcdb1\n",
    "    #Cost after one epoch\n",
    "    a0,z1,a1,z2,a2=forwardpass(xtrain)\n",
    "    ytrue=a2\n",
    "    cost1=(-(ytrain*(np.log(ytrue)))+((1-ytrain)*(np.log(1-ytrue))))\n",
    "    #for u in range(16000):\n",
    "    cost=(np.sum(crossentropy(ytrue,ytrain)))/16000.0\n",
    "     #   cost=crossentropy(ytrue[u],ytrain[u])\n",
    "      #  finalcost=finalcost+cost\n",
    "    print(\"cost\")\n",
    "    print(cost)\n",
    "    print(cost1)\n",
    "    accuracyL = accuracy(ytrue,ytrain,16000)\n",
    "    print(\"training accuracy\")\n",
    "    print(accuracyL)\n",
    "##validation\n",
    "va0,vz1,va1,vz2,va2=forwardpass(xvalidate)\n",
    "vOutput=va2\n",
    "accuracy = accuracy(vOutput,yvalidate,2000)\n",
    "costvalidate=np.sum(crossentropy(vOutput,yvalidate))/2000\n",
    "print(\"validation accuracy\")\n",
    "print(accuracy)\n",
    "print(\"validation cost\")\n",
    "print(costvalidate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
